{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled44.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: 멋진 작사가 만들기"
      ],
      "metadata": {
        "id": "wrCFmpDBmg0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObcJP4Bopag2",
        "outputId": "178e2624-11eb-4828-a5e2-e374208ff3b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iLTleNv8mdZG"
      },
      "outputs": [],
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. 데이터 읽어오기"
      ],
      "metadata": {
        "id": "cc9tzFJJn892"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os, re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/data (1)/*'\n",
        "11\n",
        " \n",
        "\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-rI_rvyoBz8",
        "outputId": "232ea5e2-ca60-49e9-c578-50c9d505ac1c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 정제"
      ],
      "metadata": {
        "id": "Rkjx3gGSqhse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
        "\n",
        "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
        "        \n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP5OX2IXqqJ_",
        "outputId": "e42dd158-6cd4-429d-fce3-216722680c15"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for some education\n",
            "Made my way into the night\n",
            "All that bullshit conversation\n",
            "Baby, can't you read the signs? I won't bore you with the details, baby\n",
            "I don't even wanna waste your time\n",
            "Let's just say that maybe\n",
            "You could help me ease my mind\n",
            "I ain't Mr. Right But if you're looking for fast love\n",
            "If that's love in your eyes\n",
            "It's more than enough\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트를 인공지능 모델에 학습시키기 위해서는 특수 문자를 제거해야 한다."
      ],
      "metadata": {
        "id": "EKcvaFx3ruDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 모든 것을 소문자로 바꾸고, 양쪽 공백을 지운다.\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백을 넣는다.\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러 개의 공백은 하나의 공백으로 바꾼다.\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다\n",
        "    sentence = sentence.strip() # 다시 양쪽 공백을 지운다\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>를 추가\n",
        "    return sentence\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5isHm8WZqu0M",
        "outputId": "8cb7ea3b-7995-4311-c546-8a3a44f895e5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어만을 다루기 때문에 특수문자를 없앴다. 그리고, 문장의 시작과 끝을 알려줬다."
      ],
      "metadata": {
        "id": "Y_CSypyVsgGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    corpus.append(preprocess_sentence(sentence))\n",
        "        \n",
        "# 10개를 확인\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtfGpQCTq2Nq",
        "outputId": "972f7309-9991-4738-af4d-7ca60f6dbf50"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "특수문자가 제거된 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "YVDfU9PFtahM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
        "\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=7000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\" # 사전에 없었던 단어를 사용할 수 있게 토큰으로 대체\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus) # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus) # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다   \n",
        "\n",
        "    total_data_text = list(tensor)\n",
        "    num_tokens = [len(tokens) for tokens in total_data_text]\n",
        "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "    maxlen = int(max_tokens)\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tw8yu-wq9at",
        "outputId": "7de7d05a-0383-45c4-ef48-bab7c7b1f9a2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 304  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2 100   6 ...   0   0   0]\n",
            " [  2 122   9 ...   0   0   0]\n",
            " [  2  74   6 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f3c9bc62490>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 전처리 해줘야 한다. 문장을 단어 단위로 나누어 컴퓨터가 이해할 수 있도록 변환시켜야 한다.\n",
        "\n",
        "단, 평균 길이보다 큰 outlier가 있을 수 있는 데, 이는 제거해줘야 한다. 컴퓨터가 읽기 힘들어 한다. 이를 해결하기 위해 maxlen을 잘 설정해야 한다."
      ],
      "metadata": {
        "id": "Yv7h01k6vF22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 값이 잘 나오는 지 확인\n",
        "\n",
        "print(tensor[:3, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v-CeS6Bq9dX",
        "outputId": "3b1f7778-7eb3-4653-9aa4-63099dc13998"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  304   28   99 4811    3    0    0    0    0]\n",
            " [   2  221   13   85  226    6  115    3    0    0]\n",
            " [   2   24   17 1087 2347    3    0    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰으로 잘 쪼개어져 있는 지 확인\n",
        "\n",
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVCWOLQzq9gr",
        "outputId": "0f1e1ada-c6ef-411b-8f05-3feb18d27d16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "tgt_input = tensor[:, 1:]  # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.  \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCshCA4wq9jP",
        "outputId": "e4fa9ece-535c-49d6-b5b1-f883e2cb26ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "길이 조절이 잘 되어 있는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "RNKSATKCxZFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "YMaodYOCxqiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어장 크기 12000이상, 총 데이터의 20%\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=34)\n"
      ],
      "metadata": {
        "id": "irO0Cgy8rYaF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 행열 잘 나왔는 지 확인\n",
        "\n",
        "print('Source Train: ', enc_train.shape)\n",
        "print('Target Train: ', dec_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngqxtJYKrYc_",
        "outputId": "feb1b209-c25b-43ee-d224-4730ae30e189"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Train:  (140599, 19)\n",
            "Target Train:  (140599, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인공지능 만들기"
      ],
      "metadata": {
        "id": "P4pEx6uHyNkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "D36R64pbylQP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        \n",
        "        self.embedding = Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 19\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "NihSA4I9rYf5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습"
      ],
      "metadata": {
        "id": "vijdNd-3yorn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "epochs = 10\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "history = model.fit(enc_train, \n",
        "          dec_train, \n",
        "          epochs=epochs,\n",
        "          batch_size=256,\n",
        "          validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3LBbnPqypxp",
        "outputId": "06178a89-3b97-4241-f8fb-c33f75151375"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "550/550 [==============================] - 173s 286ms/step - loss: 2.8043 - val_loss: 2.4827\n",
            "Epoch 2/10\n",
            "550/550 [==============================] - 157s 286ms/step - loss: 2.3915 - val_loss: 2.3263\n",
            "Epoch 3/10\n",
            "550/550 [==============================] - 165s 300ms/step - loss: 2.2255 - val_loss: 2.2082\n",
            "Epoch 4/10\n",
            "550/550 [==============================] - 157s 286ms/step - loss: 2.0568 - val_loss: 2.0963\n",
            "Epoch 5/10\n",
            "550/550 [==============================] - 165s 300ms/step - loss: 1.8750 - val_loss: 2.0052\n",
            "Epoch 6/10\n",
            "550/550 [==============================] - 157s 286ms/step - loss: 1.6867 - val_loss: 1.9348\n",
            "Epoch 7/10\n",
            "550/550 [==============================] - 157s 286ms/step - loss: 1.5057 - val_loss: 1.8812\n",
            "Epoch 8/10\n",
            "550/550 [==============================] - 157s 286ms/step - loss: 1.3439 - val_loss: 1.8546\n",
            "Epoch 9/10\n",
            "550/550 [==============================] - 165s 300ms/step - loss: 1.2069 - val_loss: 1.8357\n",
            "Epoch 10/10\n",
            "550/550 [==============================] - 165s 300ms/step - loss: 1.0958 - val_loss: 1.8336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가사 만들기"
      ],
      "metadata": {
        "id": "vnUJW2Rd6DeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나 씩 분류해놨다. 그래서 하나씩 입력하여 문장을 만들어야 한다.\n",
        "    while True:\n",
        "\n",
        "        # 입력받은 문장의 텐서를 입력합니다.\n",
        "        predict = model(test_tensor)  \n",
        "        # 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 2에서 예측된 word index를 문장 뒤에 붙입니다 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated "
      ],
      "metadata": {
        "id": "4hXrNOvc6LIn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가사 한 줄 만들어 내보자 !\n",
        "\n",
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gx8917Mz6FUf",
        "outputId": "09336838-2ece-48b7-9fd0-4e59ade366db"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you so much <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n"
      ],
      "metadata": {
        "id": "-p2v9H1A4e6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "많은 분들의 도움으로  원하는 loss 값을 쉽게 잘 찾은 것 같다. 이번 ex는 다른 것들과 다르게 train, test를 나누지 않고 train 안 에서 스스로 test를 나눠 모델링 했다는 것이 신기했다. 처음 시작할 때, 신경망이 사람을 본 따서 만들었다는 것을 의심했다. 인간을 본따서 구현한다는 것이 이질적이었다. 하지만, 코드를 직접 하나씩 써보면서 메커니즘 흐름을 따라가니 정교하게 잘만들어졌다는 생각이 들었다.\n",
        "\n",
        "아이펠에서는 2가지 과정이 있다. 자연어 처리와 영상처리.. 그 중 하나를 경험했다는 것이 고무적이다. 다음에 있을 영상처리가 기대된다."
      ],
      "metadata": {
        "id": "OqlGz_cg4hIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "출처\n",
        "\n",
        "https://github.com/PEBpung/Aiffel/blob/master/Project/Exploration/E11.%20%EC%9E%91%EC%82%AC%EA%B0%80%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb"
      ],
      "metadata": {
        "id": "dOjr7Uh_44PI"
      }
    }
  ]
}