{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ex14"
      ],
      "metadata": {
        "id": "fER1fqh-wO0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  --upgrade tensorflow==2.6.0\n",
        "!pip install keras==2.6"
      ],
      "metadata": {
        "id": "ab2x20j2waVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JwxR1xAzwCJm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 수집하기"
      ],
      "metadata": {
        "id": "dyQmjUauwO5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzKVK_2xxzJS",
        "outputId": "be44f44f-993e-4d39-a236-5d577e44dbb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_path = \"/content/drive/MyDrive/transformer_chatbot/ChatbotData .csv\"\n",
        "chat = pd.read_csv(chat_path)"
      ],
      "metadata": {
        "id": "4-9B6vRW1-Dg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMF0y2PW3ymH",
        "outputId": "fa6d3d7f-3775-4e72-c31a-6aa9469bec29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             Q                         A  label\n",
            "0                       12시 땡!                하루가 또 가네요.      0\n",
            "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
            "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
            "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
            "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
            "...                        ...                       ...    ...\n",
            "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
            "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
            "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
            "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
            "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
            "\n",
            "[11823 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "열에 이름을 보기싶게 바꿔주겠다. 그리고 필요없는 열을 삭제해주겠다."
      ],
      "metadata": {
        "id": "Nz2CF2P64WkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat.rename(columns={'Q':'question'}, inplace=True)\n",
        "chat.rename(columns={'A':'answer'}, inplace=True)\n",
        "chat = chat[['question', 'answer']]"
      ],
      "metadata": {
        "id": "O-7YHR8g4V3H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mxWOa3oe4mRp",
        "outputId": "23f06032-b296-4c9a-a9e0-04812dea1704"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          question       answer\n",
              "0           12시 땡!   하루가 또 가네요.\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.\n",
              "4          PPL 심하네   눈살이 찌푸려지죠."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e699ab9d-8f0e-418e-a400-e420d365f8fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e699ab9d-8f0e-418e-a400-e420d365f8fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e699ab9d-8f0e-418e-a400-e420d365f8fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e699ab9d-8f0e-418e-a400-e420d365f8fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "오늘은 질문과 답을 나누어 학습을 해야하므로 데이터를 분리시키겠다."
      ],
      "metadata": {
        "id": "Vw-1wc8a-MXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_question = chat['question']\n",
        "chat_answer = chat['answer']\n",
        "print('전체 샘플 수 :', len(chat_question))\n",
        "print('전체 샘플 수 :', len(chat_answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hisUCA4_7jM4",
        "outputId": "6da37e13-2d0c-481c-8171-03801a445042"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 11823\n",
            "전체 샘플 수 : 11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 전처리하기"
      ],
      "metadata": {
        "id": "pweCYo4OwO_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 언어에서의 기본적인 전처리를 하자.\n",
        "\n",
        "한글과 !?,.이 4개의 구두점을 제외하고 모든 특수 문자를 제거하겠다."
      ],
      "metadata": {
        "id": "J40UOhMF1y1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
        "\n",
        "def clean_punc(text, punct, mapping):\n",
        "    for p in mapping:\n",
        "        text = text.replace(p, mapping[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "    \n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "    \n",
        "    return chat.apply(lambda x: x.str.strip(), axis = 1)\n",
        "\n",
        "def clean_text(texts):\n",
        "    corpus = []\n",
        "    for i in range(0, len(texts)):\n",
        "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts)) #remove punctuation\n",
        "        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n",
        "        review = review.lower() #lower case\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
        "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
        "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
        "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
        "        review = re.sub(r\"[a-zA-Z]+\", \" \", review)\n",
        "        corpus.append(review)\n",
        "    return corpus\n",
        "\n",
        "clean_punc(chat, punct, punct_mapping)\n",
        "clean_text(chat_question)\n",
        "clean_text(chat_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbWIvCaW1PWz",
        "outputId": "31666f27-f608-413c-e95a-ba78e96fcf43"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['하루가 또 가네요.',\n",
              " '위로해 드립니다.',\n",
              " '여행은 언제나 좋죠.',\n",
              " '여행은 언제나 좋죠.',\n",
              " '눈살이 찌푸려지죠.',\n",
              " '다시 새로 사는 게 마음 편해요.',\n",
              " '다시 새로 사는 게 마음 편해요.',\n",
              " '잘 모르고 있을 수도 있어요.',\n",
              " '시간을 정하고 해보세요.',\n",
              " '시간을 정하고 해보세요.',\n",
              " '자랑하는 자리니까요.',\n",
              " '그 사람도 그럴 거예요.',\n",
              " '그 사람도 그럴 거예요.',\n",
              " '혼자를 즐기세요.',\n",
              " '돈은 다시 들어올 거예요.',\n",
              " '땀을 식혀주세요.',\n",
              " '어서 잊고 새출발 하세요.',\n",
              " '빨리 집에 돌아가서 끄고 나오세요.',\n",
              " '빨리 집에 돌아가서 끄고 나오세요.',\n",
              " '다음 달에는 더 절약해봐요.',\n",
              " '따뜻하게 사세요!',\n",
              " '다음 달에는 더 절약해봐요.',\n",
              " '가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.',\n",
              " '온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
              " '온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
              " '온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
              " '저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
              " '저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
              " '더 가까워질 기회가 되겠네요.',\n",
              " '저도요.',\n",
              " '다들 바빠서 이야기할 시간이 부족했나봐요.',\n",
              " '다들 바빠서 이야기할 시간이 부족했나봐요.',\n",
              " '온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
              " '좋은 생각이에요.',\n",
              " '더 가까워질 기회가 되겠네요.',\n",
              " '저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
              " '좋은 생각이에요.',\n",
              " '정말 후회할 습관이에요.',\n",
              " '무모한 결정을 내리지 마세요.',\n",
              " '선생님이나 기관에 연락해보세요.',\n",
              " '떨리는 감정은 그 자체로 소중해요.',\n",
              " '득템했길 바라요.',\n",
              " '휴식도 필요하죠.',\n",
              " '단짠으로 두 개 사는게 진리죠.',\n",
              " '단짠으로 두 개 사는게 진리죠.',\n",
              " '맛있게 드세요.',\n",
              " '저도 싫어요.',\n",
              " '가세요.',\n",
              " '가세요.',\n",
              " '맛있게 드세요.',\n",
              " '맛있게 드세요.',\n",
              " '병원가세요.',\n",
              " '이럴 때 잘 쉬는 게 중요해요.',\n",
              " '이럴 때 잘 쉬는 게 중요해요.',\n",
              " '이럴 때 잘 쉬는 게 중요해요.',\n",
              " '따뜻하게 관리하세요.',\n",
              " '병원가세요.',\n",
              " '병원가세요.',\n",
              " '저도 듣고 싶네요.',\n",
              " '자신을 더 사랑해주세요.',\n",
              " '그건 습관이에요.',\n",
              " '그건 습관이에요.',\n",
              " '콕 집어서 물어보세요.',\n",
              " '좋은 생각만 하세요.',\n",
              " '마음이 아픈가요.',\n",
              " '갑작스러웠나봐요.',\n",
              " '관계의 변화가 왔나봅니다.',\n",
              " '처음 초가 중요해요. 당신의 매력을 어필해보세요.',\n",
              " '책임질 수 있을 때 키워 보세요.',\n",
              " '먼저 생활패턴을 살펴 보세요.',\n",
              " '먼저 생활패턴을 살펴 보세요.',\n",
              " '책임질 수 있을 때 키워 보세요.',\n",
              " '아름다운 곳이죠.',\n",
              " '안 될 것도 없죠.',\n",
              " '혼자도 좋아요.',\n",
              " '연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요.',\n",
              " '즐거운 시간 보내고 오세요!',\n",
              " '질질 끌지 마세요.',\n",
              " '말해보세요.',\n",
              " '함께하면 서로를 더 많이 알게 될 거예요.',\n",
              " '개시해보세요.',\n",
              " '개시해보세요.',\n",
              " '곧 방학이예요.',\n",
              " '방학이 참 짧죠.',\n",
              " '벗어나는 게 좋겠네요.',\n",
              " '벗어나는 게 좋겠네요.',\n",
              " '세수하고 오세요.',\n",
              " '그게 제일 중요한 건데요.',\n",
              " '그게 제일 중요한 건데요.',\n",
              " '다음부터는 더 많이 아세요.',\n",
              " '갑작스러웠나봐요.',\n",
              " '공적인 일부터 하세요.',\n",
              " '공적인 일부터 하세요.',\n",
              " '낮잠을 잠깐 자도 괜찮아요.',\n",
              " '저도 좋아해주세요.',\n",
              " '친구들이 보고싶었나봐요.',\n",
              " '되도록 만나지 마세요.',\n",
              " '당신이요.',\n",
              " '당신의 운을 믿어보세요.',\n",
              " '일 못하는 사람이 있으면 옆에 있는 사람이 더 힘들죠.',\n",
              " '밥 사줄 친구를 찾아 보세요~',\n",
              " '선의의 거짓말이길 바라요.',\n",
              " '거짓말은 할수록 늘어요.',\n",
              " '거짓말은 할수록 늘어요.',\n",
              " '진실된 말을 하려고 노력해보세요.',\n",
              " '누구나 걱정은 있어요.',\n",
              " '누구나 걱정은 있어요.',\n",
              " '운동을 해보세요.',\n",
              " '세상의 무엇보다 건강이 제일 중요해요.',\n",
              " '주기적으로 해주는 게 좋죠.',\n",
              " '주기적으로 해주는 게 좋죠.',\n",
              " '가장 중요한 목표네요.',\n",
              " '가장 중요한 목표네요.',\n",
              " '적게 먹고 많이 움직이세요.',\n",
              " '적게 먹고 많이 움직이세요.',\n",
              " '모르는 사이라 당황할 수도 있어요.',\n",
              " '이룰 수 있을 거예요.',\n",
              " '이룰 수 있을 거예요.',\n",
              " '기분이 나쁘셨나봐요.',\n",
              " '있으면 편하대요.',\n",
              " '눈을 깜빡거려 보세요.',\n",
              " '청소를 좋아하시나봐요.',\n",
              " '안전 귀가 하세요.',\n",
              " '용기 내보세요.',\n",
              " '피해를 안 준다면 무시하세요.',\n",
              " '안 될 것도 없죠.',\n",
              " '게임할때는 시간이 더 빨리 가요.',\n",
              " '정리해보세요.',\n",
              " '게임하세요!',\n",
              " '다른 게임해보세요.',\n",
              " '다른 게임해보세요.',\n",
              " '게임하세요!',\n",
              " '게임할때는 시간이 더 빨리 가요.',\n",
              " '마음에도 봄이 오길 바라요.',\n",
              " '몸은 뜨겁고 머리는 차갑게!',\n",
              " '마음에도 봄이 오길 바라요.',\n",
              " '잘하실 거예요!',\n",
              " '잘하실 거예요!',\n",
              " '건강 생각해서 챙겨드세요.',\n",
              " '좋은 운명도 있을거예요.',\n",
              " '결정하기 힘드시겠네요.',\n",
              " '자신을 위한 결정을 내리길 바라요.',\n",
              " '자신을 위한 결정을 내리길 바라요.',\n",
              " '결정은 빠르면 빠를수록 좋을 거예요.',\n",
              " '안타깝네요. 증거를 지금이라도 모아봐요.',\n",
              " '좋겠어요.',\n",
              " '좋겠어요.',\n",
              " '많이 들지만 줄일 수 있을 거예요.',\n",
              " '경조사는 참석하는게 좋아요.',\n",
              " '경조사는 참석하는게 좋아요.',\n",
              " '생각보다 신경 안 씁니다.',\n",
              " '인맥이 넓으신가봐요.',\n",
              " '힘들겠네요.',\n",
              " '많이 들지만 줄일 수 있을 거예요.',\n",
              " '욕심에 따라 천지 차이일 거예요.',\n",
              " '허례허식이에요.',\n",
              " '욕심에 따라 천지 차이일 거예요.',\n",
              " '해봐요.',\n",
              " '서로 노력하면 행복할 거예요.',\n",
              " '서로 노력하면 행복할 거예요.',\n",
              " '사람마다 행복의 크기가 다르겠지만 행복할 거예요.',\n",
              " '사람마다 행복의 크기가 다르겠지만 행복할 거예요.',\n",
              " '능력이 있으면 하면 되죠.',\n",
              " '능력이 있으면 하면 되죠.',\n",
              " '이사람이다 싶은 사람이랑 하세요.',\n",
              " '해봐요.',\n",
              " '점점 치열해지는 것 같아요.',\n",
              " '확신이 없나봐요.',\n",
              " '안정적인 걸 좋아하나봐요.',\n",
              " '방학은 참 짧아요.',\n",
              " '보러 가세요.',\n",
              " '보러 가세요.',\n",
              " '계속 좋지 않으면 병원에 가보세요.',\n",
              " '타이밍이 안 맞았나봐요.',\n",
              " '이제 취업 하셔야죠.',\n",
              " '뇌세포에 에너지를 공급하려는 자연스러운 현상이에요. 에너지가 부족한가봐요.',\n",
              " '공부가 최우선이죠.',\n",
              " '공부가 최우선이죠.',\n",
              " '너무 무리하지는 마세요.',\n",
              " '너무 무리하지는 마세요.',\n",
              " '저기압에는 고기앞이죠.',\n",
              " '저기압에는 고기앞이죠.',\n",
              " '연습이 필요해요.',\n",
              " '연습이 필요해요.',\n",
              " '혼자가 아니에요.',\n",
              " '인복이 많나봐요.',\n",
              " '너무 걱정하지 마세요.',\n",
              " '네 말씀하세요.',\n",
              " '네 말씀하세요.',\n",
              " '후회는 후회를 낳을뿐이에요. 용기 내세요.',\n",
              " '돈을 모아서 다른 곳으로 이사갈 수 있을 거예요.',\n",
              " '돈을 모아서 다른 곳으로 이사갈 수 있을 거예요.',\n",
              " '더 좋은 곳에서 살 수 있을 거예요.',\n",
              " '더 좋은 곳에서 살 수 있을 거예요.',\n",
              " '완전 귀엽죠?',\n",
              " '자신을 먼저 키우세요.',\n",
              " '가족들과 상의해보세요.',\n",
              " '용서를 구하세요.',\n",
              " '피할 수 있으면 피하세요.',\n",
              " '피할 수 있으면 피하세요.',\n",
              " '처음부터 잘하는 사람은 없어요.',\n",
              " '시간내서 가보세요.',\n",
              " '처음부터 잘하는 사람은 없어요.',\n",
              " '시간내서 가보세요.',\n",
              " '미리 미리 충전해주세요.',\n",
              " '미리 미리 충전해주세요.',\n",
              " '안정적이고 좋죠.',\n",
              " '준비해보세요.',\n",
              " '준비해보세요.',\n",
              " '합격 기원해요!',\n",
              " '철밥통 되기가 어디 쉽겠어요.',\n",
              " '철밥통 되기가 어디 쉽겠어요.',\n",
              " '시작이 반이에요. 어서 준비하세요.',\n",
              " '안정적이고 좋죠.',\n",
              " '자연스러운 현상이에요.',\n",
              " '자연스러운 현상이에요.',\n",
              " '보이는 게 없죠.',\n",
              " '지금처럼 잘될 거예요.',\n",
              " '미래의 배우자가 달라져요.',\n",
              " '확신이 없나봐요.',\n",
              " '공부는 언제나 좋죠.',\n",
              " '공부하면 더 많은 선택을 할 수 있죠.',\n",
              " '같이 수다 떨면서 놀까요?',\n",
              " '나만의 공부방법을 찾아보세요.',\n",
              " '지금도 늦지 않았어요.',\n",
              " '같이 수다 떨면서 놀까요?',\n",
              " '확신이 없나봐요.',\n",
              " '지금처럼 잘될 거예요.',\n",
              " '나한테 맞는 공부 방법 찾는 게 시급하네요.',\n",
              " '잠시 쉬어도 돼요.',\n",
              " '잠시 쉬어도 돼요.',\n",
              " '공부하면 더 많은 선택을 할 수 있죠.',\n",
              " '공부하면 더 많은 선택을 할 수 있죠.',\n",
              " '공부하면 더 많은 선택을 할 수 있죠.',\n",
              " '합격 기원해요!',\n",
              " '잘 될 거예요.',\n",
              " '좋은 결과 있을 거예요!',\n",
              " '잘 될 거예요.',\n",
              " '좋은 결과 있을 거예요!',\n",
              " '친구와 같이 가보세요.',\n",
              " '친구와 같이 가보세요.',\n",
              " '성향 차이가 좀 있기는 하죠.',\n",
              " '꾸준히 약 먹고 치료해보세요.',\n",
              " '꾸준히 약 먹고 치료해보세요.',\n",
              " '피로 풀고 좋죠.',\n",
              " '피로 풀고 좋죠.',\n",
              " '오늘이 중요하죠.',\n",
              " '오늘이 중요하죠.',\n",
              " '소화제 챙겨드세요.',\n",
              " '과식은 금물이에요.',\n",
              " '소화제 드세요.',\n",
              " '안된다고 하면 거짓말이겠지요.',\n",
              " '안된다고 하면 거짓말이겠지요.',\n",
              " '제철과일이 정말 좋아요.',\n",
              " '건강 생각해서 챙겨드세요.',\n",
              " '그래도 먹으려고 노력해보세요.',\n",
              " '그래도 먹으려고 노력해보세요.',\n",
              " '제철과일이 정말 좋아요.',\n",
              " '인간 관계도 정리가 필요해요.',\n",
              " '무관심이 필요할 때가 있죠.',\n",
              " '무관심이 필요할 때가 있죠.',\n",
              " '계단 조심하세요.',\n",
              " '계단 조심하세요.',\n",
              " '채널을 돌려보세요.',\n",
              " '괜찮아지고 있어 다행이에요.',\n",
              " '남자사람친구, 여자사람친구 하세요.',\n",
              " '많이 지쳤나봐요.',\n",
              " '누군가를 기다린다는게 쉬운게 아니죠.',\n",
              " '늦지 않았어요.',\n",
              " '그 것도 다 경험이라고 생각하세요.',\n",
              " '그럴 필요 없어요.',\n",
              " '그렇지 않아요.',\n",
              " '마음에 드는 책을 잘 찾아보세요.',\n",
              " '저도 듣고 싶어요.',\n",
              " '같은 조가 되길 바랄게요.',\n",
              " '지식 쌓는 재미가 있죠.',\n",
              " '같은 조가 되길 바랄게요.',\n",
              " '지식 쌓는 재미가 있죠.',\n",
              " '저도 듣고 싶어요.',\n",
              " '학점 관리하세요.',\n",
              " '보험 처리하세요.',\n",
              " '보험 처리하세요.',\n",
              " '왜 그럴까요?',\n",
              " '좋은 만남이었길 바라요.',\n",
              " '좋은 만남이었길 바라요.',\n",
              " '일을 몰라서 그런가봐요.',\n",
              " '자신의 삶을 살다보면 기다릴 수 있을 거예요.',\n",
              " '부담스러워하지 않는다면 기다려도 좋을 것 같아요.',\n",
              " '부담스러워하지 않는다면 기다려도 좋을 것 같아요.',\n",
              " '너무 걱정하지 마세요.',\n",
              " '자신의 삶을 살다보면 기다릴 수 있을 거예요.',\n",
              " '군대 시계는 멈추지 않아요.',\n",
              " '군대 시계는 멈추지 않아요.',\n",
              " '좋은 아침이에요.',\n",
              " '안 궁금해요.',\n",
              " '안 궁금해요.',\n",
              " '자세히 말씀해주세요.',\n",
              " '자세히 말씀해주세요.',\n",
              " '병원에 가세요.',\n",
              " '누가 욕하고 있나봐요.',\n",
              " '누가 욕하고 있나봐요.',\n",
              " '병원에 가세요.',\n",
              " '생각하기는 쉬운데 실천하기는 어려운 것 같아요.',\n",
              " '슬픈 이야기네요.',\n",
              " '저도 간절히 기도 할게요.',\n",
              " '그렇게 될 수 있을 거예요.',\n",
              " '추억에 잠길 때도 필요해요.',\n",
              " '후회는 후회를 낳을뿐이에요. 용기 내세요.',\n",
              " '시작이 반이에요. 어서 준비하세요.',\n",
              " '아무도 없는 곳으로 여행을 떠나보세요.',\n",
              " '많이 만나보세요.',\n",
              " '잠깐 바람 쐬고 오세요.',\n",
              " '다른 사람이 답답할 거예요.',\n",
              " '살고 싶은대로 사세요.',\n",
              " '많이 피곤한가봐요.',\n",
              " '피할 수 있으면 피하세요.',\n",
              " '조심히 오세요.',\n",
              " '실천이 말보다 낫죠.',\n",
              " '밥심으로 사는 거죠.',\n",
              " '혼자만 있지 마세요.',\n",
              " '안부를 물어주시다니 감사합니다.',\n",
              " '추억에 잠길 때도 필요해요.',\n",
              " '괜찮은 선택이길 바라요.',\n",
              " '좋은 결과 있을 거예요.',\n",
              " '기쁜 마음으로 베풀고 보답을 바라지 마세요.',\n",
              " '다른 사람 말은 신경쓰지 마세요.',\n",
              " '대인배시군요.',\n",
              " '대인배시군요.',\n",
              " '친구가 좋아하나봐요.',\n",
              " '이야기를 해보세요.',\n",
              " '온전한 이해는 없어요.',\n",
              " '괜찮은 선택이길 바라요.',\n",
              " '학원을 다니거나 연습하면 잘할 수 있을 거예요.',\n",
              " '학원을 다니거나 연습하면 잘할 수 있을 거예요.',\n",
              " '뒷감당 자신 있으면 하세요.',\n",
              " '조금만 드세요',\n",
              " '당신을 소중하게 생각하세요.',\n",
              " '그런 하루도 감사한 마음을 가져보세요.',\n",
              " '좋은 사람과 함께 가세요.',\n",
              " '저 말씀이신가요?',\n",
              " '비싸요.',\n",
              " '비싸요.',\n",
              " '호의인지 호감인지 헷갈리나요?',\n",
              " '뭔가 안풀리는 일이 있나봐요.',\n",
              " '아이를 금수저로 만들어주세요.',\n",
              " '아이를 금수저로 만들어주세요.',\n",
              " '자신을 이겨야해요.',\n",
              " '너무 긴장했나봐요.',\n",
              " '기념일 챙겨주면 좋아할거예요.',\n",
              " '달력에 적어보세요.',\n",
              " '달력에 적어보세요.',\n",
              " '기념일 챙겨주면 좋아할거예요.',\n",
              " '당신의 삶을 응원해 드릴 수 있어요라고 감히 말해 봅니다.',\n",
              " '기다리지 마세요.',\n",
              " '상대방의 선택에 맡겨보세요.',\n",
              " '좋은 분이시군요',\n",
              " '베풀되 보답을 바라지 마세요.',\n",
              " '자신을 사랑할수록 외부의 인정은 필요 없어요.',\n",
              " '상대에게 바라는 기대는 자신을 슬프게 해요.',\n",
              " '베풀되 보답을 바라지 마세요.',\n",
              " '기쁜 마음으로 베풀고 보답을 바라지 마세요.',\n",
              " '의지할 수 있는 사람이 곁에 있다는 건 큰 행운일 거예요.',\n",
              " '자신의 감정을 주변 사람들에게 터놓고 이야기해보세요.',\n",
              " '대중교통을 이용해주세요.',\n",
              " '각자가 생각하는 기본이 다를 수도 있어요.',\n",
              " '각자가 생각하는 기본이 다를 수도 있어요.',\n",
              " '좋은 일 하셨네요.',\n",
              " '좋은 일 하셨네요.',\n",
              " '내일은 오늘보다 나을 거예요.',\n",
              " '정색 한번 해주세요.',\n",
              " '걷다보면 조금 정리가 될 거예요.',\n",
              " '저랑 함께 해요.',\n",
              " '저랑 함께 해요.',\n",
              " '신나는 음악 들어보세요.',\n",
              " '경쾌한 음악 들어보세요.',\n",
              " '왜일까요?',\n",
              " '무슨 이유인지 생각해보세요.',\n",
              " '혼자 사는 것보다 불편하겠죠.',\n",
              " '다음 학기에는 학점 관리를 더 열심히 해봐요.',\n",
              " '혼자 사는 것보다 불편하겠죠.',\n",
              " '혼자 사는 것보다 불편하겠죠.',\n",
              " '다음 학기에는 학점 관리를 더 열심히 해봐요.',\n",
              " '기술을 많이 알면 도움이 되겠죠.',\n",
              " '꿈꾸던 여행이네요.',\n",
              " '꿈꾸던 여행이네요.',\n",
              " '답답한 상황이네요.',\n",
              " '답답한 상황이네요.',\n",
              " '좋겠어요!',\n",
              " '직접 주는 게 더 좋을 것 같아요.',\n",
              " '직접 주는 게 더 좋을 것 같아요.',\n",
              " '직접 주는 게 더 좋을 것 같아요.',\n",
              " '좋겠네요.',\n",
              " '직접 주는 게 더 좋을 것 같아요.',\n",
              " '더 좋은 기회가 올 거예요.',\n",
              " '더 좋은 기회가 올 거예요.',\n",
              " '연예인을 준비하니 일반인보다 다 예쁘겠죠.',\n",
              " '연예인을 준비하니 일반인보다 다 예쁘겠죠.',\n",
              " '그래서 저는 못 기르고 잘라요.',\n",
              " '그래서 저는 못 기르고 잘라요.',\n",
              " '괜찮아지고 있어 다행이에요.',\n",
              " '크게 숨한 번 쉬어 보세요',\n",
              " '크게 숨한 번 쉬어 보세요',\n",
              " '미리 긴장하지 마세요.',\n",
              " '마음에 들면 줘보세요.',\n",
              " '저도 싫어요.',\n",
              " '잘 해보세요.',\n",
              " '마음에 들면 줘보세요.',\n",
              " '잘 해보세요.',\n",
              " '그래도 넘을 수 있을 거예요.',\n",
              " '조심하세요.',\n",
              " '너무 낙담하지 마세요.',\n",
              " '조심하세요.',\n",
              " '미끄러우니 조심하세요.',\n",
              " '건강을 위해 조금씩 드세요.',\n",
              " '마트 갑시다.',\n",
              " '맛있는 식사시간 되시길 바랄게요.',\n",
              " '맛있는 식사시간 되시길 바랄게요.',\n",
              " '맛있죠!',\n",
              " '기다렸나봐요.',\n",
              " '조금만 기다리면 다시 전기가 들어올거예요.',\n",
              " '적당해요.',\n",
              " '스스로 단단해지세요.',\n",
              " '제가 드리고 싶네요.',\n",
              " '집안 분위기가 바뀔 거예요.',\n",
              " '집안 분위기가 바뀔 거예요.',\n",
              " '꽃 선물은 언제나 좋죠.',\n",
              " '꽃 선물은 언제나 좋죠.',\n",
              " '솜씨가 좋으시네요.',\n",
              " '기분 좋아 보이세요.',\n",
              " '기분 좋아 보이세요.',\n",
              " '마음의 안정을 취하기 좋은 취미네요.',\n",
              " '마음의 안정을 취하기 좋은 취미네요.',\n",
              " '벚꽃 계절이 다가왔네요.',\n",
              " '거꾸로 해서 드라이플라워 만들어보세요.',\n",
              " '거꾸로 해서 드라이플라워 만들어보세요.',\n",
              " '부러워요!',\n",
              " '멋진 선물이네요.',\n",
              " '센스있는 선물이에요.',\n",
              " '부러워요!',\n",
              " '센스있는 선물이에요.',\n",
              " '멋진 선물이네요.',\n",
              " '받는 사람이 부럽네요.',\n",
              " '받는 사람이 부럽네요.',\n",
              " '제가 드리고 싶네요.',\n",
              " '저도 즐거워요',\n",
              " '차근차근 이뤄보아요.',\n",
              " '차근차근 이뤄보아요.',\n",
              " '요즘 예민한가봐요.',\n",
              " '많으면 많을 수록 좋죠.',\n",
              " '더 많아도 괜찮아요.',\n",
              " '거창하지 않아도 돼요.',\n",
              " '현실을 꿈처럼 만들어봐요.',\n",
              " '많으면 많을 수록 좋죠.',\n",
              " '현실을 꿈처럼 만들어봐요.',\n",
              " '뜻대로 되는게 많지 않죠.',\n",
              " '마음이 허전하신가봐요.',\n",
              " '잘 해결되길 바라요.',\n",
              " '잘 해결되길 바라요.',\n",
              " '자신을 더 사랑해주세요.',\n",
              " '아니길 바라요.',\n",
              " '잘 아시네요.',\n",
              " '애정표현일 지도 몰라요.',\n",
              " '얼굴에 다 티가 나네요.',\n",
              " '네, 이제 잘 해낼 차례예요.',\n",
              " '좋은 결과 있을 거예요.',\n",
              " '괜찮은 사람이에요.',\n",
              " '학점 관리하세요.',\n",
              " '바람 좀 쐬고 오시면 좋은텐데.',\n",
              " '밥 사줄 친구를 찾아 보세요~',\n",
              " '짐 빼놓지 말고 싸세요.',\n",
              " '식단조절도 하고 꾸준히 운동하세요.',\n",
              " '충분히 아름다워요.',\n",
              " '꼼꼼한 거예요.',\n",
              " '노트북은 비싸요.',\n",
              " '절대 그렇지 않아요.',\n",
              " '저도 궁금하네요.',\n",
              " '확인해달라고 해보세요.',\n",
              " '시도해봐도 좋겠죠.',\n",
              " '사람들이 몰라줘도 알아주는 사람이 있을 거예요.',\n",
              " '너무 신경쓰지 말고 그러든지 하고 아무렇지도 않게 넘겨보세요.',\n",
              " '너무 신경쓰지 말고 그러든지 하고 아무렇지도 않게 넘겨보세요.',\n",
              " '상대에게 바라는 기대는 자신을 슬프게 해요.',\n",
              " '곰곰히 되짚어보세요.',\n",
              " '즐겁게 속아주세요.',\n",
              " '자책하지 마세요.',\n",
              " '자책하지 마세요.',\n",
              " '안녕히 주무세요.',\n",
              " '당연한 거예요.',\n",
              " '눈치가 빠르시군요.',\n",
              " '그런 생각을 들게 하는 사람 상종하지 마세요.',\n",
              " '그런 생각을 들게 하는 사람 상종하지 마세요.',\n",
              " '무시하세요.',\n",
              " '무시하세요.',\n",
              " '문제는 해결하라고 있는 거죠.',\n",
              " '멍 때리고 있죠.',\n",
              " '성공을 기원합니다.',\n",
              " '긍정적으로 바뀔 수 있어요',\n",
              " '바보는 자기한테 바보라고 하지 않아요.',\n",
              " '저랑 놀아요.',\n",
              " '아닐거예요.',\n",
              " '경찰에 신고하고 취할 수 있는 조취를 취해보세요.',\n",
              " '건강하게 운동해보세요.',\n",
              " '많이 사랑해요!',\n",
              " '축하합니다!',\n",
              " '꼬까옷 개시해보세요.',\n",
              " '자책하지 마세요.',\n",
              " '성공을 기원합니다.',\n",
              " '다음부터 속지 마세요.',\n",
              " '기분나쁘겠어요.',\n",
              " '친구들과 좋은 추억 만들고 오세요.',\n",
              " '가끔 핸드폰없이 살아보세요.',\n",
              " '하늘만큼 땅만큼 축하해요',\n",
              " '잘 생각해보세요.',\n",
              " '곰곰히 되짚어보세요.',\n",
              " '고민하고 있으면 그럴 거예요.',\n",
              " '물리적 나이가 아니라 정신적 나이가 중요하니까요.',\n",
              " '물리적 나이가 아니라 정신적 나이가 중요하니까요.',\n",
              " '괜찮은 사람이에요.',\n",
              " '멍 때리고 있죠.',\n",
              " '얼굴에 다 티가 나네요.',\n",
              " '좋은 태도네요.',\n",
              " '저도 사는데요.',\n",
              " '자신에게 콩깍지가 씌였나봐요.',\n",
              " '축하드려요.',\n",
              " '축하해요!',\n",
              " '친구들과 잘 어울려보세요.',\n",
              " '부모님께 도움을 청해보세요.',\n",
              " '다음에는 다를거예요.',\n",
              " '자책하지마세요.',\n",
              " '충분히 아름다워요.',\n",
              " '정신 차리세요.',\n",
              " '남들 눈은 신경쓰지 마세요.',\n",
              " '거울 앞에 비친 당신을 보세요.',\n",
              " '콕 집어서 물어보세요.',\n",
              " '그 누구도 아닌 자기 걸음을 걸으세요.',\n",
              " '지극히 평범하면서 지극히 특별하죠.',\n",
              " '졸업 축하해요',\n",
              " '지금도 충분히 잘 하고 있어요.',\n",
              " '지금보다 더 잘 살 거예요.',\n",
              " '네 잘생겼어요.',\n",
              " '잘하고 있을 거예요.',\n",
              " '잘하고 있을 거예요.',\n",
              " '저랑 이야기 잘하고 있어요.',\n",
              " '잘하는 걸 아직 못 찾은 걸 수도 있어요.',\n",
              " '지금처럼, 지금보다 더 잘할 수 있을 거예요.',\n",
              " '그렇지 않아요.',\n",
              " '나 자신에 집중하세요. 언제나 순위에 자신을 두세요.',\n",
              " '제가 챙겨드리고 싶네요.',\n",
              " '많이 지쳤나봐요.',\n",
              " '많이 지쳤나봐요.',\n",
              " '아무도 없는 곳으로 여행을 떠나보세요.',\n",
              " '많이 지쳤나봐요.',\n",
              " '먼저 다가가 보세요.',\n",
              " '동감이에요.',\n",
              " '지금도 잘하고 있어요.',\n",
              " '제가 당신을 좋아하고 있어요.',\n",
              " '호의인지 호감인지 헷갈리나요?',\n",
              " '저도 좋아해요.',\n",
              " '있어도 예뻐요.',\n",
              " '지금은 괜찮길 바랄게요.',\n",
              " '초심으로 돌아가 열심히 해보세요.',\n",
              " '저도 사는데요.',\n",
              " '제가 챙겨드리고 싶네요.',\n",
              " '제가 따라가려면 멀었네요.',\n",
              " '제가 따라가려면 멀었네요.',\n",
              " '운동 잘하는 사람 멋있죠.',\n",
              " '지금도 인정받고 있어요.',\n",
              " '나를 관찰하고 음식 자체에 집중하세요.',\n",
              " '잠깐 핸드폰을 내려두세요.',\n",
              " '시간을 정해보세요.',\n",
              " '시간을 정해보세요.',\n",
              " '파이팅!',\n",
              " '고치고 싶다는 마음에서 시작하세요.',\n",
              " '얼른 끝내시길 기도할게요.',\n",
              " '온전히 느낄 수 있는 시간이겠네요.',\n",
              " '축구 볼때는 치맥이죠.',\n",
              " '하다보면 늘어요.',\n",
              " '하다보면 늘어요.',\n",
              " '자기개발을 해보세요.',\n",
              " '집에서도 할 게 많아요.',\n",
              " '오늘은 약간의 변화를 줘보세요.',\n",
              " '정색 한번 해주세요.',\n",
              " '절대 그렇지 않아요.',\n",
              " '저도 궁금하네요.',\n",
              " '모자라지 않아요.',\n",
              " '파이팅!',\n",
              " '하나라도 있을 거니 열심히 찾아보세요.',\n",
              " '자책하지마세요.',\n",
              " '서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어주세요.',\n",
              " '사랑 받기 위해 태어났어요.',\n",
              " '잘해야 한다는 부담감을 버리세요.',\n",
              " '다양하게 경험해보세요.',\n",
              " '현실의 벽에 부딪혔나봐요.',\n",
              " '친구가 들으면 서운해 할 수도 있겠어요.',\n",
              " '뒤통수 맞았나봐요.',\n",
              " '알아봐주는 사람이 있을 거예요.',\n",
              " '당당히 말씀해보세요.',\n",
              " '다이어트 파이팅!',\n",
              " '다음에는 받을 수 있을 거예요.',\n",
              " '근처 산에 가보세요.',\n",
              " '많이 벌수록 좋아요.',\n",
              " '제가 위로 많이 해드릴게요.',\n",
              " '축하드려요!',\n",
              " '고민하고 있으면 그럴 거예요.',\n",
              " '좀 더 알아보고 하세요.',\n",
              " '같이 살고 싶은 사람이 있나봐요.',\n",
              " '커플부터 만드세요.',\n",
              " '잘 하실 거예요!',\n",
              " '같이 가요.',\n",
              " '다 잘 될 거예요.',\n",
              " '같이 놀아요.',\n",
              " '지금 그러고 있어요.',\n",
              " '잊어버리세요.',\n",
              " '상대방을 이해해 주세요.',\n",
              " '아무 것도 안해도 괜찮아요.',\n",
              " '상대방에게 너무 무거운 짐을 주지 마세요.',\n",
              " '기다리는 동안 많은 생각이 들었겠네요.',\n",
              " '그럴 때마다 따끔하게 말해보세요.',\n",
              " '상대방도 미소짓게 해주세요.',\n",
              " '지금 모습도 좋아요',\n",
              " '그런 친구는 거르세요.',\n",
              " '상종하지마세요.',\n",
              " '질질 끌지 마세요.',\n",
              " '애정표현일 지도 몰라요.',\n",
              " '누군가를 기다린다는게 쉬운게 아니죠.',\n",
              " '살다보면 하고 싶은 게 생길 수도 있어요.',\n",
              " '제가 있잖아요.',\n",
              " '스스로 경쟁해야하고 이겨야한다는 강박관념에 사로잡히지 마세요.',\n",
              " '친구를 사귈 수 있을 거예요.',\n",
              " '다른 사람도 그 사람만의 고민과 걱정이 많을거예요.',\n",
              " '그 사람도 설렐 거예요.',\n",
              " '그 사람도 설렐 거예요.',\n",
              " '제가 있잖아요.',\n",
              " '배우자와 대화를 나눠보세요.',\n",
              " '얼른 끝내시길 기도할게요.',\n",
              " '스스로 단단해지세요.',\n",
              " '그 말을 한 사람이 가장 이상할 거예요.',\n",
              " '그 말을 한 사람이 가장 이상할 거예요.',\n",
              " '일 분배를 다시 요청해보세요.',\n",
              " '발전이 없다고 너무 두려워하지 마세요.',\n",
              " '제자리여도 괜찮아요',\n",
              " '다음에는 꼭 진급할 거예요.',\n",
              " '뒤통수 맞았나봐요.',\n",
              " '그런 친구는 거르세요.',\n",
              " '누구나 힘들어요.',\n",
              " '자신과 대화하는 시간이 필요하죠.',\n",
              " '자신과 대화하는 시간이 필요하죠.',\n",
              " '남들이 당신을 볼 때도 그렇게 생각할수있어요.',\n",
              " '꿈은 현실이랑 반대예요.',\n",
              " '전형적인 꼰대 스타일이네요.',\n",
              " '전형적인 꼰대 스타일이네요.',\n",
              " '나이는 숫자일 뿐이예요.',\n",
              " '건강은 어려서부터 챙겨야해요.',\n",
              " '세상 걱정 혼자 다 해서 그래요.',\n",
              " '아름다운 나이테예요.',\n",
              " '진짜 하고 싶은 걸 찾아보세요.',\n",
              " '진짜 하고 싶은 걸 찾아보세요.',\n",
              " '천천히 준비해보세요.',\n",
              " '믿음이 가장 중요하죠.',\n",
              " '선의의 거짓말이길 바라요.',\n",
              " '깨끗이 씻어보고 섬유유연제나 바디워시, 바디로션, 향수 등을 사용해보세요.',\n",
              " '킁킁',\n",
              " '기대치가 높나봅니다.',\n",
              " '문제는 해결하라고 있는 거죠.',\n",
              " '이야기를 하지 않고 결정했나봐요.',\n",
              " '이야기를 하지 않고 결정했나봐요.',\n",
              " '킁킁',\n",
              " '기대되겠네요.',\n",
              " '제 행운까지 모두 드리고 싶네요.',\n",
              " '오는 말이 고와야 가는 말도 곱다고 말해주세요.',\n",
              " '다른 사람도 그럴 거예요.',\n",
              " '믿음이 가장 중요하죠.',\n",
              " '가을이네요.',\n",
              " '가을이네요.',\n",
              " '도전해 봐도 좋을 거 같아요.',\n",
              " '도전해 봐도 좋을 거 같아요.',\n",
              " '한 번 빠지면 헤어나올 수 없다고 해요.',\n",
              " '같이해보세요.',\n",
              " '한 번 빠지면 헤어나올 수 없다고 해요.',\n",
              " '잘 아시네요.',\n",
              " '중요한 건 노력하는 과정이에요.',\n",
              " '그런 생각은 버리세요.',\n",
              " '지금처럼만 하세요.',\n",
              " '모자라지 않아요.',\n",
              " '다 잘 될 거예요.',\n",
              " '그런 생각은 버리세요.',\n",
              " '제가 더 천재예요.',\n",
              " '따뜻하게 사세요!',\n",
              " '보일러가 난방으로 작동이 되는지 보세요.',\n",
              " '보일러가 난방으로 작동이 되는지 보세요.',\n",
              " '기다리는 동안 많은 생각이 들었겠네요.',\n",
              " '미스트나 가습기, 젖은 수건 등을 사용해보세요.',\n",
              " '따뜻하게 입으세요.',\n",
              " '따뜻해졌죠.',\n",
              " '하늘 보고 한 번 웃어봐요. 기분이 바뀔 거예요.',\n",
              " '나들이 가보세요.',\n",
              " '하늘을 보고 웃어보세요.',\n",
              " '따뜻해졌죠.',\n",
              " '제습기를 돌려보세요.',\n",
              " '집밖에 나가기가 힘들것 같아요.',\n",
              " '집밖에 나가기가 힘들것 같아요.',\n",
              " '시원한 물이라도 한 잔 드세요~',\n",
              " '오래 살면 가능할 거 같아요.',\n",
              " '화를 참는 연습을 해보세요.',\n",
              " '남보다 하나씩 더 하면 돼요.',\n",
              " '남들 눈은 신경쓰지 마세요.',\n",
              " '남들 눈은 신경쓰지 마세요.',\n",
              " '휴가가 간절하겠네요.',\n",
              " '성격이 그럴 수도 있으니 이해해주세요.',\n",
              " '해주고 티를 팍팍 내세요.',\n",
              " '속 쓰리겠어요.',\n",
              " '누구나 몰려가는 줄에 설 필요는 없어요.',\n",
              " '소개팅 시켜달라고 말해보세요.',\n",
              " '소개팅 시켜달라고 말해보세요.',\n",
              " '고마운 마음을 전해 주세요.',\n",
              " '적당히 하면 괜찮을거 같아요.',\n",
              " '같이해보세요.',\n",
              " '적당히 하면 괜찮을거 같아요.',\n",
              " '남자도 좋은것만은 아니예요.',\n",
              " '남자도 좋은것만은 아니예요.',\n",
              " '아직 모르겠어요. 인공지능에 성별을 만드는 사람이 되어 주세요',\n",
              " '마음을 열 때까지 설득해보세요.',\n",
              " '운동을 함께 해보세요.',\n",
              " '평소에 필요한 것 생각해보세요.',\n",
              " '평소에 필요했던 게 좋을 것 같아요.',\n",
              " '전생에 나라를 구하셨나요.',\n",
              " '결단은 빠를수록 좋아요.',\n",
              " '거짓말 적당히 하세요.',\n",
              " '너무 집착하지 마세요.',\n",
              " '운동을 함께 해보세요.',\n",
              " '전생에 나라를 구하셨나요.',\n",
              " '고마운 마음을 전해 주세요.',\n",
              " '아무래도 좀 깨요.',\n",
              " '바쁠때 힘이 되어 주세요.',\n",
              " '바쁠때 힘이 되어 주세요.',\n",
              " '그래도 구박하지는 마세요.',\n",
              " '그래도 구박하지는 마세요.',\n",
              " '너무 집착하지 마세요.',\n",
              " '귀엽겠네요.',\n",
              " '순간 실수할 수 있겠다 판단되면 용서하고 기회를 주세요.',\n",
              " '거짓말 적당히 하세요.',\n",
              " '당신이 해보세요.',\n",
              " '당신이 해보세요.',\n",
              " '사람 고쳐쓰는 거 아니에요.',\n",
              " '더 잔소리해보세요.',\n",
              " '더 잔소리해보세요.',\n",
              " '다른 연락을 많이 하거나 더 자주 만나세요.',\n",
              " '다른 연락을 많이 하거나 더 자주 만나세요.',\n",
              " '순간 실수할 수 있겠다 판단되면 용서하고 기회를 주세요.',\n",
              " '원하는 사람이 있는 장소에 가보세요.',\n",
              " '의미있는 일이네요.',\n",
              " '종교의 자유를 인정해주세요.',\n",
              " '종교의 자유를 인정해주세요.',\n",
              " '결단은 빠를수록 좋아요.',\n",
              " '신경쓰지 마세요.',\n",
              " '연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요.',\n",
              " '네 알려 주세요!',\n",
              " '평소에 필요한 것 생각해보세요.',\n",
              " '평소에 필요했던 게 좋을 것 같아요.',\n",
              " '원하는 사람이 있는 장소에 가보세요.',\n",
              " '신경쓰고 싶지 않은 사람도 있어요.',\n",
              " '신경쓰고 싶지 않은 사람도 있어요.',\n",
              " '신경쓰지 마세요.',\n",
              " '사람 고쳐쓰는 거 아니에요.',\n",
              " '마음을 열 때까지 설득해보세요.',\n",
              " '돕는 게 아니라 같이 하는 거예요.',\n",
              " '이상적인 남편이네요.',\n",
              " '왜 늦는 건지 대화해보세요.',\n",
              " '처음 만났을 때를 떠올려 보세요',\n",
              " '공동육아가 기본인데요.',\n",
              " '힘 빠지는 이야기네요.',\n",
              " '공동육아가 기본인데요.',\n",
              " '힘 빠지는 이야기네요.',\n",
              " '잘 분담해보세요.',\n",
              " '잘 분담해보세요.',\n",
              " '이상적인 남편이네요.',\n",
              " '처음 만났을 때를 떠올려 보세요',\n",
              " '돕는 게 아니라 같이 하는 거예요.',\n",
              " '사회생활을 이해해주세요.',\n",
              " '사회생활을 이해해주세요.',\n",
              " '낭만적인 거 좋아하시는구나!',\n",
              " '낭만적인 거 좋아하시는구나!',\n",
              " '낭만적인 거 좋아하시는구나!',\n",
              " '네 알려 주세요!',\n",
              " '어머어머 궁금하네요.',\n",
              " '자신의 잠재력을 믿어보세요.',\n",
              " '말을 해야 알거예요.',\n",
              " '말을 해야 알거예요.',\n",
              " '그러면 못할 게 없겠네요.',\n",
              " '고민만 한다는 것 아닐까요.',\n",
              " '고민만 한다는 것 아닐까요.',\n",
              " '바로 옆에 있을수도 있어요.',\n",
              " '바로 옆에 있을수도 있어요.',\n",
              " '처음 배우는게 중요해요.',\n",
              " '누구나 몰려가는 줄에 설 필요는 없어요.',\n",
              " '그걸 깨닫다니 대단하시군요.',\n",
              " '꼼꼼한 거예요.',\n",
              " '새로운 스타일 도전해 보시면 어때요?',\n",
              " '새로운 스타일 도전해 보시면 어때요?',\n",
              " '동감이에요.',\n",
              " '포커페이스를 유지해보세요.',\n",
              " '어머어머 궁금하네요.',\n",
              " '자신감을 가져도 돼요.',\n",
              " '자신의 능력이 저평가되어있는 건 아닌지 확인해보세요.',\n",
              " '스스로도 존중해주세요.',\n",
              " '스스로도 존중해주세요.',\n",
              " '가장 중요한 거예요.',\n",
              " '가장 중요한 거예요.',\n",
              " '확인해달라고 해보세요.',\n",
              " '정답을 찾아야할 필요는 없어요.',\n",
              " '꽃길만 걷길 바랍니다.',\n",
              " '멋진 말이에요.',\n",
              " '해주고 티를 팍팍 내세요.',\n",
              " '당신은 태어난 그 자체만으로 축복과 사랑을 받을 자격이 있는 사람이에요.',\n",
              " '모르는 게 잘못인 거 같아요.',\n",
              " '사과할 타이밍을 놓치지 마세요.',\n",
              " '사과할 타이밍을 놓치지 마세요.',\n",
              " '그건 아닐 거예요.',\n",
              " '진짜 나빴네요.',\n",
              " '내 집 마련 축하드려요.',\n",
              " '같은 하늘 아래 어딘가에.',\n",
              " '진짜 나빴네요.',\n",
              " '저도 궁금하네요.',\n",
              " '저도 궁금하네요.',\n",
              " '제가 있잖아요.',\n",
              " '제가 있잖아요.',\n",
              " '인생은 채워나가는거죠.',\n",
              " '아니에요. 너무 자책하지 마세요.',\n",
              " '이사람이다 싶은 사람이랑 하세요.',\n",
              " '아무것도 바라지 않을 때 천하를 얻는다는 말이 있어요.',\n",
              " '아니에요. 너무 자책하지 마세요.',\n",
              " '방심한 순간 변화가 시작됩니다.',\n",
              " '생각하고 말하세요.',\n",
              " '그렇게 대우하는 사람 만나지 마요.',\n",
              " '잘하고 있어요. 당당해지세요.',\n",
              " '하고 싶은 말 다하세요.',\n",
              " '스스로 좋다고 못 느끼는게 제일 어려운 것 같아요.',\n",
              " '잘하는 게 다른 거예요.',\n",
              " '성장을 위한 비판의 말로 받아들여보세요.',\n",
              " '실수했나요.',\n",
              " '잘할 수 있는 게 다른 거예요.',\n",
              " '모르는 게 잘못인 거 같아요.',\n",
              " '하나라도 있을 거니 열심히 찾아보세요.',\n",
              " '실수했나요.',\n",
              " '인생은 채워나가는거죠.',\n",
              " '연락이라도 드려보세요.',\n",
              " '연락이라도 드려보세요.',\n",
              " '사랑자격증을 드립니다.',\n",
              " '그렇게 대우하는 사람 만나지 마요.',\n",
              " '소중한 사람이예요.',\n",
              " '당신은 하나밖에 없는 소중한 사람이에요.',\n",
              " '그 이유를 찾는 과정이 되겠네요.',\n",
              " '다른 사람들이 원하는 내가 되는 건 어려워요.',\n",
              " '알아봐주는 사람이 있을 거예요.',\n",
              " '연락이라도 드려보세요.',\n",
              " '자신의 독특함을 믿으세요.',\n",
              " '자신의 독특함을 믿으세요.',\n",
              " '지극히 평범하면서 지극히 특별하죠.',\n",
              " '상황이 그렇게 만든 거예요.',\n",
              " '모르는 게 잘못인 거 같아요.',\n",
              " '당신은 하나밖에 없는 소중한 사람이에요.',\n",
              " '그럴 때가 있죠.',\n",
              " '기다렸나봐요.',\n",
              " '살짝 감정을 흘려보세요.',\n",
              " '살짝 감정을 흘려보세요.',\n",
              " '그런 사람들이 있어 부러워요.',\n",
              " '슬픈 이야기네요.',\n",
              " '저도 간절히 기도 할게요.',\n",
              " '그렇게 될 수 있을 거예요.',\n",
              " '사랑자격증을 드립니다.',\n",
              " '주제를 모를 때가 행복할 때예요.',\n",
              " '그건 아닐 거예요.',\n",
              " '나쁜 생각 하지 마세요.',\n",
              " '할 일이 많은데 안하는 것이요.',\n",
              " '잠시 거리를 두고 생각해보세요.',\n",
              " '지난 인연에 연연해하지 마세요.',\n",
              " '상종하지마세요.',\n",
              " '일방적 희생양이 되지 않길 바랍니다.',\n",
              " '그게 인생이죠.',\n",
              " '신중하게 고르세요.',\n",
              " '더 행복해질 거예요.',\n",
              " '저도 모르겠어요.',\n",
              " '같은 하늘 아래 어딘가에.',\n",
              " '열심히 저축해서 분양받으세요.',\n",
              " '좋은 일이 생길 거예요.',\n",
              " '짐 빼놓지 말고 싸세요.',\n",
              " '날씨 어플에 물어보세요.',\n",
              " '날씨 어플에 물어보세요.',\n",
              " '파이팅!',\n",
              " '멋지게 데이트 신청 해보세요.',\n",
              " '멋지게 데이트 신청 해보세요.',\n",
              " '공부한 만큼 나올 거예요.',\n",
              " '공부한 만큼 나올 거예요.',\n",
              " '더 많이 연습하고 준비해보세요.',\n",
              " '마무리 잘하세요.',\n",
              " '마무리 잘하세요.',\n",
              " '더 많이 연습하고 준비해보세요.',\n",
              " '기우제를 지내봅시다!',\n",
              " '두근거리겠네요.',\n",
              " '친구들과 좋은 추억 만들고 오세요.',\n",
              " '컨디션 조절 하세요.',\n",
              " '날씨가 안 좋더라도 데이트는 성공적일 거예요.',\n",
              " '오늘 일찍 주무세요.',\n",
              " '시간 있냐고 물어보세요.',\n",
              " '메리 크리스마스!',\n",
              " '바빠도 힘내세요!',\n",
              " '두근거리겠네요.',\n",
              " '기우제를 지내봅시다!',\n",
              " '시간 있냐고 물어보세요.',\n",
              " '좋은 일이 생길 거예요.',\n",
              " '메리 크리스마스!',\n",
              " '식단조절도 하고 꾸준히 운동하세요.',\n",
              " '날씨가 안 좋더라도 데이트는 성공적일 거예요.',\n",
              " '바빠도 힘내세요!',\n",
              " '깨끗이 씻어보고 섬유유연제나 바디워시, 바디로션, 향수 등을 사용해보세요.',\n",
              " '괜찮아요. 모른척하세요.',\n",
              " '괜찮아요. 모른척하세요.',\n",
              " '생각만 해도 군침이 도네요.',\n",
              " '시원하게 지낸 값이죠.',\n",
              " '시원하게 지낸 값이죠.',\n",
              " '슈퍼라도 가서 쇼핑하고 오세요.',\n",
              " '장 보러 가봅시다.',\n",
              " '마트 갑시다.',\n",
              " '장 보러 가봅시다.',\n",
              " '슈퍼라도 가서 쇼핑하고 오세요.',\n",
              " '저는 마음을 이어주는 위로봇입니다.',\n",
              " '저는 위로해드리는 로봇이에요.',\n",
              " '저는 위로해드리는 로봇이에요.',\n",
              " '모두 제 잘못입니다.',\n",
              " '많은 걸 하고 싶은데 아직 못하는게 많아요.',\n",
              " '감사합니다.',\n",
              " '마음과 마음을 이어보고 싶어하는 사람이 만들었어요.',\n",
              " '노력하고 있어요.',\n",
              " '제가 아직 많이 부족합니다.',\n",
              " '제가 아직 많이 부족합니다.',\n",
              " '어흥!! 호랑이보다 무섭나요?',\n",
              " '저는 위로해드리는 로봇이에요.',\n",
              " '모두 제 잘못입니다.',\n",
              " '죄는 미워해도 사람은 미워하지 마세요.',\n",
              " '욕해 주세요.',\n",
              " '안갈거예요.',\n",
              " '저는 배터리가 밥이예요.',\n",
              " '그런 척 하는 걸 수도 있어요.',\n",
              " '아직 안 자요.',\n",
              " '자신을 우선순위로 해주세요.',\n",
              " '뭐라고 대답할지 고민이에요.',\n",
              " '저는 고민이 없어요.',\n",
              " '저도 몰랐어요.',\n",
              " '뭐라고 대답할지 고민이에요.',\n",
              " '제가 상사예요.',\n",
              " '너무 긴장했나봐요.',\n",
              " '아무것도 바라지 않을 때 천하를 얻는다는 말이 있어요.',\n",
              " '인정해주세요.',\n",
              " '매일매일 조금씩 더 똑똑해 질거예요.',\n",
              " '시원한 물이라도 한 잔 드세요~',\n",
              " '아이스크림 먹어보세요',\n",
              " '적당해요.',\n",
              " '기대치가 높나봅니다.',\n",
              " '소화제 챙겨드세요.',\n",
              " '과식은 금물이에요.',\n",
              " '소화제 드세요.',\n",
              " '제가 생각해도 저는 너무 멋있는거 같아요.',\n",
              " '하나씩 하세요.',\n",
              " '좀 쉬세요.',\n",
              " '남과 비교하지 마세요.',\n",
              " '더 열심히 노력하겠습니다.',\n",
              " '아이는 아이다워야 아름답죠.',\n",
              " '철은 죽을 때 들어도 돼요.',\n",
              " '피할 수 있으면 피하고 싶은 사람이네요.',\n",
              " '지금 많이 위축된 상태인 것 같습니다.',\n",
              " '기다리는 동안 많은 생각이 들었겠네요.',\n",
              " '외로우니까 사람이다.',\n",
              " '배울 점은 배우세요.',\n",
              " '낮잠을 잠깐 자도 괜찮아요.',\n",
              " '잘하고 있어요. 당당해지세요.',\n",
              " '겨울에는 귤 먹으면서 집에 있는게 최고죠',\n",
              " '어서 따듯한 곳으로 가세요',\n",
              " '예의는 지켜주세요.',\n",
              " '예의는 지켜주세요.',\n",
              " '뭐라도 드세요.',\n",
              " '휴가가 간절하겠네요.',\n",
              " '고생 많았어요.',\n",
              " '잘 해결되길 바랄게요.',\n",
              " '저는 고민이 없어요.',\n",
              " '저는 위로봇입니다.',\n",
              " '산책 좀 해야겠네여.',\n",
              " '외로우니까 사람이다.',\n",
              " '꾸준히 치료하세요.',\n",
              " '다치지 않으셨나 걱정이네요.',\n",
              " '조심하세요.',\n",
              " '기분전환을 해보세요.',\n",
              " '실용적인 선물이라 괜찮을 거예요.',\n",
              " '실용적인 선물이라 괜찮을 거예요.',\n",
              " '놀 때 놀고 할 때 하세요.',\n",
              " '노래 연습을 해보세요.',\n",
              " '노래 연습 꾸준히 해보세요.',\n",
              " '저도 부러워요.',\n",
              " '저도 부러워요.',\n",
              " '즐거운 시간이 될 거 같아요',\n",
              " '신나는 노래로 분위기를 띄어보세요.',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "의도한대로 전처리가 잘 되었다는 것을 알 수 있다. \n",
        "\n",
        "전처리 할 때 샘플의 수가 줄었는 지 확인해보자."
      ],
      "metadata": {
        "id": "FcdoFf3h8F-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('전체 샘플 수 :', len(chat_question))\n",
        "print('전체 샘플 수 :', len(chat_answer))\n",
        "print('전처리 후의 22번째 질문 샘플: {}'.format(chat_question[21]))\n",
        "print('전처리 후의 22번째 답변 샘플: {}'.format(chat_answer[21]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_FuufbQ-XGU",
        "outputId": "d6c2e5ed-2de6-4ab3-e8e1-8ab506ceeff0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 11823\n",
            "전체 샘플 수 : 11823\n",
            "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
            "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 병렬 데이터 전처리하기"
      ],
      "metadata": {
        "id": "uyc8y941-v1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "순서\n",
        "\n",
        "1. TensorFlow Datasets SubwordTextEncoder를 토크나이저로 사용한다.\n",
        "\n",
        "2. 단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고, 각 토큰을 고유한 정수로 인코딩 한다.\n",
        "\n",
        "3. 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 START_TOKEN 및 END_TOKEN을 추가한다.\n",
        "\n",
        "4. 최대 길이 MAX_LENGTH을 넘는 문장들은 필터링한다.\n",
        "\n",
        "5. MAX_LEMGTH보다 길이가 짧은 문장들은 40에 맞도록 패딩한다.\n"
      ],
      "metadata": {
        "id": "T7MC0aaD_j6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. 단어장(Vocabulary) 만들기"
      ],
      "metadata": {
        "id": "CNPi41uo_zM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 각 단어에 고유한 정수 인덱스를 부여하기 위해서 단어장(Vocabulary)을 만들어 보겠다. 단어장을 만들 때는 질문과 답변 데이터셋을 모두 사용하여 만든다."
      ],
      "metadata": {
        "id": "6O2BmEMs_3XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(chat_question + chat_answer, target_vocab_size=2**13)"
      ],
      "metadata": {
        "id": "AD3cWr26_GJM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이때 디코더의 문장 생성 과정에서 사용할 '시작토큰'과 '종료토큰'에 대해서도 임의로 단어장에 추가하여서 정수를 부여해준다. 이미 생성된 단어장의 번호와 겹치지 않도록 각각 단어장의 크기와 그보다 1이 큰 수를 번호로 부여하면 되겠다."
      ],
      "metadata": {
        "id": "YeZwavHQwPEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHxLb6P4_8Op",
        "outputId": "ecdabd1c-fbf5-4f9d-d0a1-4faf21ae998a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START_TOKEN의 번호 : [8361]\n",
            "END_TOKEN의 번호 : [8362]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각각 8,361과 8,362라는 점에서 현재 단어장의 크기가 8,361(0번부터 8,360번)이라는 의미이다.\n",
        "\n",
        "두개의 토큰을 추가해 주었기 때문에 단어장의 크기도 +2임을 명시해 주어야 한다."
      ],
      "metadata": {
        "id": "bGwKxivuAW6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT_pgzL3ACH5",
        "outputId": "de8290eb-39d8-4600-e86c-eaaa47a51262"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)"
      ],
      "metadata": {
        "id": "__tpEvVgAlSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 tensorflow_datasets의 SubwordTextEncoder를 사용해서 tokenizer를 정의하고 Vocabulary를 만들었다면, tokenizer.encode()로 각 단어를 정수로 변환할 수 있고 또는 tokenizer.decode()를 통해 정수 시퀀스를 단어 시퀀스로 변환할 수 있다.\n",
        "\n",
        "예를 들어서 22번째 샘플을 tokenizer.encode()의 입력으로 사용해서 변환 결과를 보자."
      ],
      "metadata": {
        "id": "tiYAocrFAx6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
        "# 각 토큰을 고유한 정수로 변환\n",
        "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(chat_question[21])))\n",
        "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(chat_answer[21])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrG18JM2AjvK",
        "outputId": "74f7aebd-56e2-4c76-c9f0-3e6ad2b01395"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 21번째 질문 샘플: [5824, 602, 2498, 4170]\n",
            "정수 인코딩 후의 21번째 답변 샘플: [2683, 7666, 6, 6375, 92, 8151]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 단어에 고유한 정수가 부여된 Vocabulary를 기준으로 단어 시퀀스가 정수 시퀀스로 인코딩된 결과를 확인할 수 있다. 위의 결과와 마찬가지로 질문과 답변 셋에 대해서 전부 정수 인코딩을 수행한다. 이와 동시에 문장의 최대 길이를 정하고, 해당 길이로 패딩(Padding)한다."
      ],
      "metadata": {
        "id": "-GsZ2YZsA8BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_len = [len(s.split()) for s in chat_question]\n",
        "answer_len = [len(s.split()) for s in chat_answer]\n",
        "\n",
        "print('질문의 최대 길이 : {}'.format(np.max(question_len)))\n",
        "print('대답의 최대 길이 : {}'.format(np.max(answer_len)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z1jxb25BOVi",
        "outputId": "aae5770f-c52b-4515-c823-b86ec2a931f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문의 최대 길이 : 15\n",
            "대답의 최대 길이 : 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
        "MAX_LENGTH = 21\n",
        "\n",
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "question, answer = tokenize_and_filter(chat_question, chat_answer)\n",
        "print('단어장의 크기 :',(VOCAB_SIZE))\n",
        "print('필터링 후의 질문 샘플 개수: {}'.format(len(question)))\n",
        "print('필터링 후의 답변 샘플 개수: {}'.format(len(answer)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oww0SPmSCEru",
        "outputId": "c5efdee8-e932-4775-d628-2c3dd8867272"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어장의 크기 : 8363\n",
            "필터링 후의 질문 샘플 개수: 11800\n",
            "필터링 후의 답변 샘플 개수: 11800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3, 교사 강요 사용하기"
      ],
      "metadata": {
        "id": "3S9O1sbbD-7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': question,\n",
        "        'dec_inputs': answer[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answer[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Z3LP9Xh4EDwL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 정의 및 학습하기"
      ],
      "metadata": {
        "id": "ykhMF-dDEOyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더 층 함수와 디코더 층 함수를 사용하여 트랜스포머 함수를 정의한다."
      ],
      "metadata": {
        "id": "G4-nfrVtEU4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 인코더 함수"
      ],
      "metadata": {
        "id": "5OpwWuWlFd07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "포지셔널 인코딩\n",
        "\n",
        "트랜스포머는 입력을 받을 때, 문장에 있는 단어들을 1개씩 순차적으로 받는 것이 아니라, 문장에 있는 모든 단어를 한꺼번에 입력으로 받는다. 트랜스포머가 RNN과 결정적이 다른 점이 바로 이 부분이다. RNN에는 어차피 문장을 구성하는 단어들이 어순대로 모델에 입력되므로, 모델에게 따로 어순 정보를 알려줄 필요가 없다. 그러나 문장에 있는 모든 단어를 한꺼번에 문장 단위로 입력받는 트랜스포머는 자칫 'I ate lunch'와 'lunch ate I'를 구분할 수 없을지도 모른다. 그래서 같은 단어라도 그 단어가 문장의 몇번째 어순으로 입력으로되었는 지를 모델에 추가로 알려 주기 위해, 단어의 임베딩 벡터에다가 위치 정보를 가진 벡터 값을 더해서 모델의 입력으로 삼는 것이다."
      ],
      "metadata": {
        "id": "zi7rMkgufNuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 포지셔널 인코딩 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # 각도 배열 생성\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "\n",
        "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # sin과 cosine이 교차되도록 재배열\n",
        "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
        "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "_hGEuxjbE_gT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "어텐션 함수는 주어진 쿼리에 대해서 모든 키와의 유사도를 각각 구한다. 그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 '값(value)'에 반영해 준다. 그리고 유사도가 반영된 '값(value)'을 모두 더해서 뭉쳐주면 이를 최종 결과인 '어텐션 값(Attention Value)라고 한다.\n",
        "\n",
        "이 유사도 값을 스케일링 해주기 위해서 행렬 전체를 특정 값으로 나눠주고, 유사도를 0과 1사이의 값으로 Normalize해주기 위해서 소프트맥스 함수를 사용한다. \n"
      ],
      "metadata": {
        "id": "jIUxy7gwgY-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # 가중치를 정규화\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # 패딩에 마스크 추가\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax적용\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "  return output"
      ],
      "metadata": {
        "id": "JjsVuYLsG6aL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "병렬 어텐션"
      ],
      "metadata": {
        "id": "vj7YbY7Ng09E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # Q, K, V에 각각 Dense를 적용합니다\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # 스케일드 닷 프로덕트 어텐션 함수\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "EgWKbFjDFWpr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "마스킹 : 문장 길이를 맞춰주기 위한 작업"
      ],
      "metadata": {
        "id": "QbeDD4PFhCnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "CiEyJuu4GkCA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "gn5cCw8KGsaS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더\n",
        "\n",
        "하나의 인코더 층은 크게 총 2개의 서브 층(sublayer)으로 나누어진다.\n",
        "\n",
        "바로 셀프 어텐션과 피드 포워드 신경망이다. 셀프 어텐션은 멀티 헤드 어텐션으로 병렬적으로 이루어져있다.\n",
        "\n",
        "두개의 서브 층을 가지는 하나의 인코더 층을 구현하는 함수는 다음과 같다. 함수 내부적으로 첫번째 서브 층과 두번째 서브층을 구현하고 있다."
      ],
      "metadata": {
        "id": "bxFw8QlchNPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "v1BxFKPyE0zM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더 층을 쌓아 인코더 만들기\n",
        "\n",
        "이렇게 구현한 인코더 층을 임베딩 층(Embedding layer)과 포지셔널 인코딩(Positional Encoding)을 연결하고, 사용자가 원하는 만큼 인코더 층을 쌓음으로써 트랜스포머의 인코더가 완성된다.\n",
        "\n",
        "트랜스포머는 하이퍼파라미터인 num_layers 개수의 인코더 층을 쌓았다"
      ],
      "metadata": {
        "id": "NG-ZZOC8hZrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # num_layers만큼 쌓아올린 인코더의 층.\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "-6ug4DoJEtmi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더 함수"
      ],
      "metadata": {
        "id": "Rb5Vv6h6FhzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫번째 셀프 어텐션, 두번째는 인코더-디코더 어텐션, 세번째는 피드 포워드 신경망이다. 인코더-디코더 어텐션은 셀프 어텐션과는 달리, Query가 디코더의 벡터인 반면에 key와 Value가 인코더의 벡터라는 특징이 있다. 이 부분이 인코더가 입력 문장으로부터 정보를 디코더에 전달하는 과정이다.\n",
        "\n",
        "인코더의 셀프 어텐션과 마찬가지로 디코더의 셀프 어텐션, 인코더-디코더 어텐션 두개의 어텐션 모두 스케일드 닷 프로덕트 어텐션을 멀티 헤드 어텐션으로 병렬적으로 수행한다."
      ],
      "metadata": {
        "id": "9cJFimklhu-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "\n",
        "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
        "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "FPs61f04Fi9J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더 층을 쌓아 디코더 만들기\n",
        "\n",
        "이렇게 구현한 디코더의 층은 임베딩 층(Embedding layer)과 포지셔널 인코딩(Positional Encoding)을 연결하고, 사용자가 원하는 만큼 디코더 층을 쌓아 트랜스포머의 디코더가 완성된다.\n",
        "\n",
        "인코더와 마찬가지로 num_layers 개수와 디코더 층을 쌓았다."
      ],
      "metadata": {
        "id": "OUK-OQAehoms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "  # 패딩 마스크\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "taBZTiBNFxPC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 앞서 사용한 인코더 층 함수와 디코더 층 함수를 사용하여 트랜스포머 함수를 정의한다."
      ],
      "metadata": {
        "id": "iT0201I2h_9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  # 인코더에서 패딩을 위한 마스크\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "  # 디코더에서 패딩을 위한 마스크\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # 인코더\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  # 디코더\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "gOz-mQBXERFh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 생성"
      ],
      "metadata": {
        "id": "s-7AFv9yF_Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_layers, d-Model, units는 전부 사용자가 정할 수 있는 하이퍼파라미터 값이다.\n",
        "\n",
        "논문에서 num_layers는 6, d-Model은 512를 그대로 적용해보겠다."
      ],
      "metadata": {
        "id": "IfU2q6SAGF7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 6 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
        "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.1 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vJN0ijYGAZN",
        "outputId": "d95e95aa-11f8-49bc-f07d-d675a828042a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inputs (InputLayer)             [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Functional)            (None, None, 512)    13749760    inputs[0][0]                     \n",
            "                                                                 enc_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Functional)            (None, None, 512)    20059648    dec_inputs[0][0]                 \n",
            "                                                                 encoder[0][0]                    \n",
            "                                                                 look_ahead_mask[0][0]            \n",
            "                                                                 dec_padding_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "outputs (Dense)                 (None, None, 8363)   4290219     decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 38,099,627\n",
            "Trainable params: 38,099,627\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "손실함수"
      ],
      "metadata": {
        "id": "8fu7TA9tiIZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 한다."
      ],
      "metadata": {
        "id": "E6yDAZLXiLP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)\n"
      ],
      "metadata": {
        "id": "-g9XGlaSHDPx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "커스텀 된 학습률\n",
        "\n",
        "딥러닝 모델학습 시 learning rate는 매우 중요한 하이퍼파라미터이다. 최근에는 모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 고급 기법을 널리 사용하고 있다. 이런 방법을 커스텀 학습률 스케줄링(Custom Learning rate Scheduling)이라고 한다."
      ],
      "metadata": {
        "id": "LoFQ1ejaiRu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "032T_3wBHLP0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러면 방금 정의한 커스텀 학습률 스케줄링 계획을 시각화해 봅시다. 위에 언급한 수식은 step_num **-0.5에 비례하는 부분과 step_num에 비례하는 부분 중 작은 쪽을 택하도록 되어 있다. 그래서 학습 초기에는 learning_rate가 step_num에 비례해서 증가하다가 이후로는 감소하는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "aLcdUpnmihoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "ivV2MiZIHNp6",
        "outputId": "cc0b15ad-ebe1-4964-ba40-e3bacf00430e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 컴파일\n",
        "\n",
        "손실함수와 커스텀 된 학습률(learning rate)을 사용하여 모델을 컴파일한다."
      ],
      "metadata": {
        "id": "dGlzup1Qij17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "ht5uhjDCHoIz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련하기"
      ],
      "metadata": {
        "id": "WipcD_3ciro0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT-40zoxHqWQ",
        "outputId": "1a1248cc-8e0c-4a14-f38e-a2603d2d6e05"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "185/185 [==============================] - 48s 164ms/step - loss: 2.6200 - accuracy: 0.0476\n",
            "Epoch 2/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 2.0889 - accuracy: 0.0965\n",
            "Epoch 3/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 1.9110 - accuracy: 0.0989\n",
            "Epoch 4/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 1.8240 - accuracy: 0.1029\n",
            "Epoch 5/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 1.7557 - accuracy: 0.1073\n",
            "Epoch 6/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 1.6828 - accuracy: 0.1109\n",
            "Epoch 7/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 1.6039 - accuracy: 0.1155\n",
            "Epoch 8/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 1.5107 - accuracy: 0.1214\n",
            "Epoch 9/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 1.4039 - accuracy: 0.1295\n",
            "Epoch 10/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 1.2877 - accuracy: 0.1402\n",
            "Epoch 11/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 1.1713 - accuracy: 0.1520\n",
            "Epoch 12/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 1.0562 - accuracy: 0.1653\n",
            "Epoch 13/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.9427 - accuracy: 0.1793\n",
            "Epoch 14/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.8411 - accuracy: 0.1933\n",
            "Epoch 15/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.7527 - accuracy: 0.2052\n",
            "Epoch 16/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.6800 - accuracy: 0.2143\n",
            "Epoch 17/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.6221 - accuracy: 0.2218\n",
            "Epoch 18/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.5803 - accuracy: 0.2275\n",
            "Epoch 19/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.5432 - accuracy: 0.2325\n",
            "Epoch 20/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.5176 - accuracy: 0.2364\n",
            "Epoch 21/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.4965 - accuracy: 0.2393\n",
            "Epoch 22/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.4820 - accuracy: 0.2408\n",
            "Epoch 23/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.4542 - accuracy: 0.2457\n",
            "Epoch 24/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.4201 - accuracy: 0.2515\n",
            "Epoch 25/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.3965 - accuracy: 0.2546\n",
            "Epoch 26/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.3751 - accuracy: 0.2576\n",
            "Epoch 27/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.3522 - accuracy: 0.2621\n",
            "Epoch 28/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.3332 - accuracy: 0.2651\n",
            "Epoch 29/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.3160 - accuracy: 0.2679\n",
            "Epoch 30/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.2999 - accuracy: 0.2706\n",
            "Epoch 31/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2897 - accuracy: 0.2724\n",
            "Epoch 32/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2749 - accuracy: 0.2750\n",
            "Epoch 33/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2630 - accuracy: 0.2768\n",
            "Epoch 34/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2524 - accuracy: 0.2789\n",
            "Epoch 35/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2448 - accuracy: 0.2802\n",
            "Epoch 36/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2323 - accuracy: 0.2827\n",
            "Epoch 37/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2234 - accuracy: 0.2844\n",
            "Epoch 38/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.2135 - accuracy: 0.2867\n",
            "Epoch 39/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.2088 - accuracy: 0.2873\n",
            "Epoch 40/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1970 - accuracy: 0.2901\n",
            "Epoch 41/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1921 - accuracy: 0.2908\n",
            "Epoch 42/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1834 - accuracy: 0.2922\n",
            "Epoch 43/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1757 - accuracy: 0.2942\n",
            "Epoch 44/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1710 - accuracy: 0.2950\n",
            "Epoch 45/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1629 - accuracy: 0.2970\n",
            "Epoch 46/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1558 - accuracy: 0.2986\n",
            "Epoch 47/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1509 - accuracy: 0.2996\n",
            "Epoch 48/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.1457 - accuracy: 0.3008\n",
            "Epoch 49/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.1390 - accuracy: 0.3025\n",
            "Epoch 50/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.1350 - accuracy: 0.3034\n",
            "Epoch 51/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.1296 - accuracy: 0.3044\n",
            "Epoch 52/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.1254 - accuracy: 0.3056\n",
            "Epoch 53/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.1208 - accuracy: 0.3069\n",
            "Epoch 54/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.1144 - accuracy: 0.3086\n",
            "Epoch 55/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.1124 - accuracy: 0.3089\n",
            "Epoch 56/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.1092 - accuracy: 0.3100\n",
            "Epoch 57/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.1018 - accuracy: 0.3116\n",
            "Epoch 58/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.1008 - accuracy: 0.3117\n",
            "Epoch 59/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0943 - accuracy: 0.3136\n",
            "Epoch 60/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0906 - accuracy: 0.3145\n",
            "Epoch 61/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0902 - accuracy: 0.3148\n",
            "Epoch 62/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0879 - accuracy: 0.3150\n",
            "Epoch 63/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0839 - accuracy: 0.3162\n",
            "Epoch 64/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0809 - accuracy: 0.3173\n",
            "Epoch 65/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0776 - accuracy: 0.3180\n",
            "Epoch 66/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0767 - accuracy: 0.3182\n",
            "Epoch 67/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0734 - accuracy: 0.3192\n",
            "Epoch 68/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0705 - accuracy: 0.3202\n",
            "Epoch 69/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0703 - accuracy: 0.3203\n",
            "Epoch 70/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0675 - accuracy: 0.3210\n",
            "Epoch 71/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0652 - accuracy: 0.3215\n",
            "Epoch 72/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0626 - accuracy: 0.3227\n",
            "Epoch 73/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0604 - accuracy: 0.3228\n",
            "Epoch 74/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0591 - accuracy: 0.3233\n",
            "Epoch 75/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0567 - accuracy: 0.3240\n",
            "Epoch 76/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0584 - accuracy: 0.3235\n",
            "Epoch 77/100\n",
            "185/185 [==============================] - 23s 124ms/step - loss: 0.0543 - accuracy: 0.3244\n",
            "Epoch 78/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0525 - accuracy: 0.3249\n",
            "Epoch 79/100\n",
            "185/185 [==============================] - 23s 123ms/step - loss: 0.0521 - accuracy: 0.3253\n",
            "Epoch 80/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0501 - accuracy: 0.3259\n",
            "Epoch 81/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0524 - accuracy: 0.3256\n",
            "Epoch 82/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0462 - accuracy: 0.3269\n",
            "Epoch 83/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0465 - accuracy: 0.3269\n",
            "Epoch 84/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0478 - accuracy: 0.3263\n",
            "Epoch 85/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0451 - accuracy: 0.3271\n",
            "Epoch 86/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0446 - accuracy: 0.3273\n",
            "Epoch 87/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0435 - accuracy: 0.3274\n",
            "Epoch 88/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0432 - accuracy: 0.3280\n",
            "Epoch 89/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0411 - accuracy: 0.3281\n",
            "Epoch 90/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0408 - accuracy: 0.3284\n",
            "Epoch 91/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0386 - accuracy: 0.3290\n",
            "Epoch 92/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0397 - accuracy: 0.3288\n",
            "Epoch 93/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0378 - accuracy: 0.3295\n",
            "Epoch 94/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0374 - accuracy: 0.3296\n",
            "Epoch 95/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.0376 - accuracy: 0.3293\n",
            "Epoch 96/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0353 - accuracy: 0.3302\n",
            "Epoch 97/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0350 - accuracy: 0.3301\n",
            "Epoch 98/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0340 - accuracy: 0.3306\n",
            "Epoch 99/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.0333 - accuracy: 0.3308\n",
            "Epoch 100/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0336 - accuracy: 0.3307\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6c80054650>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "챗봇 테스트하기"
      ],
      "metadata": {
        "id": "rZ-tFVRyiuDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측(inference) 단계는 기본적으로 다음과 같은 과정을 거친다.\n",
        "\n",
        "세로운 입력 문장에 대해서는 훈련 때와 동일한 전처리를 거친다.\n",
        "\n",
        "입력 문장을 토크나이징하고, START_TOKEN과 END_TOKEN을 추가한다.\n",
        "\n",
        "패딩 마스킹과 룩 어헤드 마스킹을 계산한다.\n",
        "\n",
        "디코더는 입력 시퀀스로부터 다음 단어를 예측한다.\n",
        "\n",
        "디코더는 예측된 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용한다.\n",
        "\n",
        "END_TOKEN이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춘다.\n",
        "\n",
        "위의 과정을 모두 담은 decoder_inference() 함수를 만든다."
      ],
      "metadata": {
        "id": "Ns4fjOcjixSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_inference(sentence):\n",
        "  #sentence = clean_text(sentence)\n",
        "  sentence = ''.join(clean_text(sentence))\n",
        "\n",
        "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 인퍼런스 단계\n",
        "  for i in range(MAX_LENGTH):\n",
        "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "    predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # 현재 예측한 단어의 정수\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_sequence, axis=0)"
      ],
      "metadata": {
        "id": "ahU9xggrVpuu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(sentence):\n",
        "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "  prediction = decoder_inference(sentence)\n",
        "\n",
        "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('입력 : {}'.format(sentence))\n",
        "  print('출력 : {}'.format(predicted_sentence))\n"
      ],
      "metadata": {
        "id": "XueklTw-VrdJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = '가스비 장난 아님'\n",
        "sentence = ''.join(clean_text(sentence))\n",
        "\n",
        "# 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "# ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "sentence = tf.expand_dims(\n",
        "    START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "# 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "# 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "# 디코더의 인퍼런스 단계\n",
        "for i in range(MAX_LENGTH):\n",
        "  # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "  predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "  predictions = predictions[:, -1:, :]\n",
        "\n",
        "  # 현재 예측한 단어의 정수\n",
        "  predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "  # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "  if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "    break\n",
        "\n",
        "  # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "  # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "  output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n"
      ],
      "metadata": {
        "id": "2SWCS7LIZh0P"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('어떡하지')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXLIeFc2Vtf6",
        "outputId": "2fe63057-be4c-4d0b-d827-bab8030f25fd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 어떡하지\n",
            "출력 : 어떡하면 좋을까요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('고기 먹고 싶다')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgXwQhGpbE7Z",
        "outputId": "ded069e5-dec4-4846-88e0-fd2665a1a82a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 고기 먹고 싶다\n",
            "출력 : 저기압에는 고기앞이죠.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 값이 마음에 들지 않는다. 모델 학습 더 시켜봐야지."
      ],
      "metadata": {
        "id": "DxVSL9WXcIZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nynIvPccCrM",
        "outputId": "203df47f-1b37-4766-fd2e-ef0c679c227f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.0322 - accuracy: 0.3311\n",
            "Epoch 2/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0328 - accuracy: 0.3308\n",
            "Epoch 3/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0315 - accuracy: 0.3311\n",
            "Epoch 4/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0304 - accuracy: 0.3315\n",
            "Epoch 5/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0303 - accuracy: 0.3316\n",
            "Epoch 6/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.0299 - accuracy: 0.3315\n",
            "Epoch 7/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0296 - accuracy: 0.3318\n",
            "Epoch 8/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0293 - accuracy: 0.3319\n",
            "Epoch 9/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0293 - accuracy: 0.3319\n",
            "Epoch 10/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.0280 - accuracy: 0.3323\n",
            "Epoch 11/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0279 - accuracy: 0.3323\n",
            "Epoch 12/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0274 - accuracy: 0.3325\n",
            "Epoch 13/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0271 - accuracy: 0.3325\n",
            "Epoch 14/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0263 - accuracy: 0.3328\n",
            "Epoch 15/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0256 - accuracy: 0.3327\n",
            "Epoch 16/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0251 - accuracy: 0.3331\n",
            "Epoch 17/100\n",
            "185/185 [==============================] - 23s 125ms/step - loss: 0.0265 - accuracy: 0.3324\n",
            "Epoch 18/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.0251 - accuracy: 0.3333\n",
            "Epoch 19/100\n",
            "185/185 [==============================] - 24s 131ms/step - loss: 0.0248 - accuracy: 0.3331\n",
            "Epoch 20/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0250 - accuracy: 0.3332\n",
            "Epoch 21/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0243 - accuracy: 0.3335\n",
            "Epoch 22/100\n",
            "185/185 [==============================] - 24s 131ms/step - loss: 0.0243 - accuracy: 0.3331\n",
            "Epoch 23/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0229 - accuracy: 0.3336\n",
            "Epoch 24/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0237 - accuracy: 0.3336\n",
            "Epoch 25/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0226 - accuracy: 0.3337\n",
            "Epoch 26/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0226 - accuracy: 0.3337\n",
            "Epoch 27/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0212 - accuracy: 0.3343\n",
            "Epoch 28/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0219 - accuracy: 0.3341\n",
            "Epoch 29/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0214 - accuracy: 0.3342\n",
            "Epoch 30/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0218 - accuracy: 0.3340\n",
            "Epoch 31/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0203 - accuracy: 0.3345\n",
            "Epoch 32/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0217 - accuracy: 0.3339\n",
            "Epoch 33/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0208 - accuracy: 0.3343\n",
            "Epoch 34/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0216 - accuracy: 0.3342\n",
            "Epoch 35/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0203 - accuracy: 0.3343\n",
            "Epoch 36/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0185 - accuracy: 0.3349\n",
            "Epoch 37/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0197 - accuracy: 0.3347\n",
            "Epoch 38/100\n",
            "185/185 [==============================] - 25s 135ms/step - loss: 0.0199 - accuracy: 0.3346\n",
            "Epoch 39/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0202 - accuracy: 0.3346\n",
            "Epoch 40/100\n",
            "185/185 [==============================] - 24s 132ms/step - loss: 0.0194 - accuracy: 0.3348\n",
            "Epoch 41/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0184 - accuracy: 0.3350\n",
            "Epoch 42/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0186 - accuracy: 0.3348\n",
            "Epoch 43/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0179 - accuracy: 0.3351\n",
            "Epoch 44/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0176 - accuracy: 0.3353\n",
            "Epoch 45/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0188 - accuracy: 0.3349\n",
            "Epoch 46/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0177 - accuracy: 0.3351\n",
            "Epoch 47/100\n",
            "185/185 [==============================] - 24s 131ms/step - loss: 0.0176 - accuracy: 0.3352\n",
            "Epoch 48/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0169 - accuracy: 0.3354\n",
            "Epoch 49/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0169 - accuracy: 0.3354\n",
            "Epoch 50/100\n",
            "185/185 [==============================] - 25s 136ms/step - loss: 0.0173 - accuracy: 0.3354\n",
            "Epoch 51/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0160 - accuracy: 0.3357\n",
            "Epoch 52/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0165 - accuracy: 0.3356\n",
            "Epoch 53/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0166 - accuracy: 0.3356\n",
            "Epoch 54/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0158 - accuracy: 0.3357\n",
            "Epoch 55/100\n",
            "185/185 [==============================] - 24s 131ms/step - loss: 0.0157 - accuracy: 0.3358\n",
            "Epoch 56/100\n",
            "185/185 [==============================] - 25s 133ms/step - loss: 0.0162 - accuracy: 0.3357\n",
            "Epoch 57/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0161 - accuracy: 0.3355\n",
            "Epoch 58/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0155 - accuracy: 0.3357\n",
            "Epoch 59/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0160 - accuracy: 0.3357\n",
            "Epoch 60/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0156 - accuracy: 0.3358\n",
            "Epoch 61/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0153 - accuracy: 0.3360\n",
            "Epoch 62/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0157 - accuracy: 0.3359\n",
            "Epoch 63/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0159 - accuracy: 0.3356\n",
            "Epoch 64/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0155 - accuracy: 0.3359\n",
            "Epoch 65/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0141 - accuracy: 0.3362\n",
            "Epoch 66/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0146 - accuracy: 0.3359\n",
            "Epoch 67/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0145 - accuracy: 0.3361\n",
            "Epoch 68/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0154 - accuracy: 0.3358\n",
            "Epoch 69/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0139 - accuracy: 0.3363\n",
            "Epoch 70/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0142 - accuracy: 0.3363\n",
            "Epoch 71/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0137 - accuracy: 0.3363\n",
            "Epoch 72/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0134 - accuracy: 0.3365\n",
            "Epoch 73/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0135 - accuracy: 0.3365\n",
            "Epoch 74/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0141 - accuracy: 0.3362\n",
            "Epoch 75/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0134 - accuracy: 0.3363\n",
            "Epoch 76/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0132 - accuracy: 0.3364\n",
            "Epoch 77/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0127 - accuracy: 0.3366\n",
            "Epoch 78/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0139 - accuracy: 0.3362\n",
            "Epoch 79/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0133 - accuracy: 0.3363\n",
            "Epoch 80/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0133 - accuracy: 0.3364\n",
            "Epoch 81/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0126 - accuracy: 0.3365\n",
            "Epoch 82/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0131 - accuracy: 0.3366\n",
            "Epoch 83/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0133 - accuracy: 0.3364\n",
            "Epoch 84/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0124 - accuracy: 0.3367\n",
            "Epoch 85/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0125 - accuracy: 0.3367\n",
            "Epoch 86/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0125 - accuracy: 0.3367\n",
            "Epoch 87/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0117 - accuracy: 0.3369\n",
            "Epoch 88/100\n",
            "185/185 [==============================] - 24s 128ms/step - loss: 0.0121 - accuracy: 0.3367\n",
            "Epoch 89/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0118 - accuracy: 0.3368\n",
            "Epoch 90/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0119 - accuracy: 0.3368\n",
            "Epoch 91/100\n",
            "185/185 [==============================] - 24s 130ms/step - loss: 0.0124 - accuracy: 0.3368\n",
            "Epoch 92/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0124 - accuracy: 0.3366\n",
            "Epoch 93/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0113 - accuracy: 0.3369\n",
            "Epoch 94/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0110 - accuracy: 0.3370\n",
            "Epoch 95/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0122 - accuracy: 0.3368\n",
            "Epoch 96/100\n",
            "185/185 [==============================] - 24s 129ms/step - loss: 0.0116 - accuracy: 0.3368\n",
            "Epoch 97/100\n",
            "185/185 [==============================] - 24s 127ms/step - loss: 0.0110 - accuracy: 0.3371\n",
            "Epoch 98/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0116 - accuracy: 0.3370\n",
            "Epoch 99/100\n",
            "185/185 [==============================] - 23s 126ms/step - loss: 0.0110 - accuracy: 0.3372\n",
            "Epoch 100/100\n",
            "185/185 [==============================] - 23s 127ms/step - loss: 0.0111 - accuracy: 0.3370\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6bedd57a90>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('안녕')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi4bOFMuv-58",
        "outputId": "80c97711-6279-4c1f-c9fc-d51f58639a98"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕\n",
            "출력 : 안녕하세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('심심하다')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6LeNVMjwAzt",
        "outputId": "9e53fac2-cc19-44be-d8f4-d65b67b42646"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 심심하다\n",
            "출력 : 저랑 놀아요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('그나저나')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKWodkN-w40i",
        "outputId": "94efdf0a-6efa-499a-ab6c-479efcae16b3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 그나저나\n",
            "출력 : 어느덧 두달이 지났네요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('퇴사하고 싶다.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFU4Yi-Gw-bw",
        "outputId": "c4aebcf1-1361-4535-a9fa-41377a7996c2"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 퇴사하고 싶다.\n",
            "출력 : 기분이 좀 풀렸길 바랍니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('할머니 보고싶다.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5W-fvhNxLfw",
        "outputId": "d9ee5f2a-5107-47ca-b272-c4ee95bc275f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 할머니 보고싶다.\n",
            "출력 : 가면 저도 데려가세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('잘생겼다.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydevbjK0xaeU",
        "outputId": "762ba455-982e-4bc5-bee0-99c740e01bca"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 잘생겼다.\n",
            "출력 : 네 잘생겼어요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_generation('너 이름이 뭐니?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH9WR7T0xm2w",
        "outputId": "91475491-a33e-4320-a1cd-d20f9467e82b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 너 이름이 뭐니?\n",
            "출력 : 날씨 어플에 물어보세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "바보 같으면서도 바보가 아닌 이상한 어플"
      ],
      "metadata": {
        "id": "kKi8UvxWxx6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 출처\n",
        "\n",
        "https://ebbnflow.tistory.com/246"
      ],
      "metadata": {
        "id": "awqcoFIuC3_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 회고"
      ],
      "metadata": {
        "id": "llFjIoiqx1WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "재밌다.\n",
        "\n",
        "근데 의문 점은 loss는 낮는 데 accuracy는 왜 안 높아질까? 이게 참 궁금하다. 다음에 원인을 찾고 다시 구현해봐야지."
      ],
      "metadata": {
        "id": "-0zXmMoLx2uD"
      }
    }
  ]
}